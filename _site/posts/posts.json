[
  {
    "path": "posts/welcome/",
    "title": "Welcome to VAST Challenge - Mini Challenge 1",
    "description": "This blog is for the assignment submission for VAST Challenge - Mini Challenge 1.",
    "author": [
      {
        "name": "Lim Jin Ru (Alethea)",
        "url": {}
      }
    ],
    "date": "2021-07-23",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-23T01:23:54+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-07-23-vast-challenge-2021-mini-challenge-1/",
    "title": "VAST Challenge 2021: Mini Challenge 1",
    "description": {},
    "author": [
      {
        "name": "Lim Jin Ru (Alethea)",
        "url": {}
      }
    ],
    "date": "2021-07-23",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\nBackground\r\nLiterature Review, Objective and Motivation\r\nData Preparation\r\nAnswers\r\nReferences\r\n\r\nOverview\r\nThis assignment is based on the mini-challenge in VAST Challenge 2021. The selected challenge topic for this assignment is Mini-Challenge 1.\r\nIn the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.\r\nIn January 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.\r\nThe Protectors of Kronos (POK) is a political activist movement stemmed from concerns about contamination from drilling at the Tiskele Bend gas fields. An international agency has tested water from the Tiskele River both upstream and downstream of the Tiskele Bend gas fields and confirmed that the presence of contaminants is consistent with pollution from Hyper Acidic Substrate Removal, a gas drilling technique employed by GAStech at the Tiskele Bend fields and these test results have also been published in several international journals.\r\nMy role is to use visual analytics to help law enforcement from Kronos and Tethys discover the relationships among the people and organizations.\r\nBackground\r\nMini-Challenge 1 looks at the relationships and conditions that led up to the kidnapping. As an analyst, I’d be analysing a set of current and historical news reports, resumes of numerous GAStech employees and email headers from two weeks of internal GAStech company email to identify the complex relationships among all of these people and organizations.\r\nLiterature Review, Objective and Motivation\r\nIn the literature review conducted of the 2014 VAST Challenge submissions covering the same crime case but with different questions, it was observed that many of the visualizations were quite informative and had their own strengths but there were some limitations or areas to improve as well.\r\nSome areas that can be further improved on:\r\nUnidirectional tree diagrams were used to display relationships, but such diagrams lack the ability to show the dynamics of how multiple parties act as connectors of the various organizations/people\r\nCluttered network diagrams with labels obscured by arrows\r\nSome of the network diagrams for emails between GAStech employees were not easy to visualize some embedded relationships (e.g. relationships within the department)\r\nThe proposed visualizations will attempt to overcome some of these limitations.\r\nThe participants used a variety of tools for their visualizations. For this assignment, the approach will be to utilize R programming solely as there are numerous visualization packages available in the R environment with useful functions and great adaptability. There are also many new packages constantly being pushed out in the R community. For this assignment, some of the newer R packages will also be explored and applied, including corporaexplorer published in 2021, patchwork published in 2020 and LDAvis published in 2015.\r\nData Preparation\r\nData extraction, wrangling and data preparation were performed with R, primarily with tidyverse methods.\r\nThis code chunk installs and launches relevant R packages:\r\n\r\n\r\npackages = c('tidytext', 'widyr', 'wordcloud', 'DT', 'ggwordcloud', 'dplyr', 'textplot', \r\n             'lubridate', 'hms', 'tidyverse', 'tidygraph', 'ggraph', 'igraph', 'scales', \r\n             'tidyr', 'purrrlyr', 'RColorBrewer', 'ggplot2', 'htmlwidgets', 'plotly', \r\n             'extrafont', 'stringr', 'corporaexplorer', 'stringi', 'stringr', 'tibble',\r\n             'rvest', 'readr', 'purrr', 'future', 'tictoc', 'lda', 'topicmodels', \r\n             'LDAvis', 'tidyHeatmap', 'utf8','tm', 'readtext', 'data.table', 'textreadr',\r\n             'SnowballC', 'RColorBrewer', 'ggplot2', 'wordcloud', 'biclust', 'cluster', \r\n             'igraph', 'fpc', 'igraph', 'ggiraph', 'visNetwork', 'networkD3',\r\n             'lubridate', 'anytime', 'quanteda', 'reshape2', 'jsonlite', 'sentimentr', \r\n             'textdata', 'rlist', 'viridisLite', 'viridis', 'topicmodels', 'readxl', \r\n             'ggthemes', 'ggalluvial', 'clock', 'hrbrthemes', 'patchwork', 'anytime')\r\nfor (p in packages){\r\n if(!require(p, character.only=T)) {\r\n install.packages(p)\r\n }\r\n library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nReading the articles from the News Articles folder and compiling them in a tibble format and saving it in rds format:\r\n\r\nnews <- \"News Articles/\"\r\n\r\nread_folder <- function(infolder) {\r\n  tibble(file = dir(infolder, full.names= TRUE)) %>%\r\n  mutate(text = map(file, read_lines)) %>%\r\n  transmute(id = basename(file), text) %>%\r\n  unnest(text)\r\n  \r\nraw_text <- tibble(folder = dir(news, full.names = TRUE)) %>%\r\n  mutate(folder_out = map(folder, read_folder)) %>%\r\n  unnest(cols = c(folder_out)) %>%\r\n  transmute(newsgroup = basename(folder), id, text)\r\nwrite_rds(raw_text, \"rds/news.rds\")  \r\n\r\nPerforming EDA on the news articles by news group:\r\n\r\n\r\nraw_text %>% \r\n  group_by(newsgroup) %>%\r\n  summarize(messages = n_distinct(id)) %>%\r\n  ggplot(aes(messages, newsgroup)) + \r\n  geom_col(fill = \"lightblue\") + \r\n  labs(y = NULL)\r\n\r\n\r\n\r\nCleaning the raw text data and saving it as cleaned_text:\r\n\r\n\r\ncleaned_text <- raw_text %>%\r\n  group_by(newsgroup, id) %>%\r\n  filter(cumsum(text ==\"\") > 0, \r\n         cumsum(str_detect(text, \"^--\")) == 0) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nFilter out undesired portion of the cleaned_text depending on context:\r\n\r\n\r\ncleaned_text <- cleaned_text %>%\r\n                    filter(!str_detect(text, \"PUBLISHED\\\\:\")) %>%\r\n                    filter(!str_detect(text, \"LOCATION\\\\:\")) %>%\r\n                    filter(!str_detect(text, \"SOURCE\\\\:\")) %>%\r\n                    filter(!str_detect(text, \"AUTHOR\\\\:\")) %>%\r\n                    filter(!str_detect(text, \"\\\\d+:\")) %>%\r\n                    filter(!str_detect(text, \"aandacht\")) %>%\r\n                    filter(!str_detect(text, \"TITLE\\\\:\")) %>% \r\n                    filter(!str_detect(text, \"continue\\\\reading\"))\r\n\r\n\r\n\r\nTokenizing the cleaned text:\r\n\r\n\r\nusenet_words <- cleaned_text %>%\r\n  unnest_tokens(word, text) %>%\r\n  filter(str_detect(word, \"[a-z']$\"), !word %in% stop_words$word, !str_detect(word, \"title\")\r\n  )\r\n\r\n\r\n\r\nGrouping words based on newsgroup:\r\n\r\n\r\nwords_by_newsgroup <- usenet_words %>%\r\n  count(newsgroup, word, sort = TRUE) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nAnswers\r\nQn 1: Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources?\r\nTo determine whether a news article is a primary or derivative source, a few visualisations would be used.\r\nThere are many characteristics that differentiate a primary and derivative source. A derivative source is any record that relies on other records for its information.\r\nAs derivative sources are second-hand account of events, they will cover information from the primary source, often providing additional analysis and interpretation. A correlation analysis of documents published by the news companies can help give us clues in the primary and derivative source relationships.\r\nFirst, the correlation is set very low to allow us to explore the overall correlation between all the news sources of the different companies.\r\n\r\nTethys News is an outlier and needed a closer inspection for analysis.\r\nA corpus exploration tool created from ‘corporaexplorer’ package in R was used. Tethys News’ articles have an update timestamp instead of just a publication date which suggests that their reporting are quite recent/close to the event. News that are written or made during or close to the time of the event is a characteristic of a primary source. Hence, Tethys News is likely to be a primary source.\r\n\r\nA high correlation of the news companies suggests that that the articles between the two companies are highly similar and that they may either both be derivative sources as they analyse and interpret each other’s work or that one is a primary source and the other is a derivative source as the latter aims to analyse and interpret the primary source, hence referencing a substantial amount of content from the primary source to facilitate analysis/interpretation. A correlation of 0.70 is selected to evaluate the relationships.\r\n\r\nA number of clusters were formed with Centrum Sentinel and Modern Rubicon being quite far from the group, suggesting that they have higher likelihood of being of a primary source.\r\nFor the others with high correlation with each other, they are likely to be derivative sources.\r\nFor instance, Athena Speaks and Central Bulletin have a correlation of more than 0.8. This suggests that their news articles are highly similar.\r\nCodes for the correlation visualizations\r\nPerforming a pairwise correlation:\r\n\r\n\r\nnewsgroup_cors <- words_by_newsgroup %>%\r\n  pairwise_cor(newsgroup, word, n, sort = TRUE)\r\n\r\n\r\n\r\nVisualising the pairwise correlation at r = 0.1:\r\n\r\n\r\nset.seed(123)\r\n\r\nnewsgroup_cors %>%\r\n  filter(correlation > .1) %>%\r\n  graph_from_data_frame() %>%\r\n  ggraph(layout = \"nicely\") +\r\n  geom_edge_link(aes(alpha = correlation, width = correlation), color=\"grey\") +\r\n  #geom_node_point(size = 4, color = \"lightblue\") +\r\n  geom_node_text(aes(label = vapply(name, str_wrap, character(1), width = 10)), colour = \"blue\", size =4) +\r\n\r\ntheme_void()\r\n\r\n\r\n\r\nVisualising the pairwise correlation at r =0.7:\r\n\r\n\r\nset.seed(123)\r\n\r\nnewsgroup_cors %>%\r\n  filter(correlation > .7) %>%\r\n  graph_from_data_frame() %>%\r\n  ggraph(layout = \"nicely\") +\r\n  geom_edge_link(aes(alpha = correlation, width = correlation), color=\"grey\") +\r\n  geom_node_point(size = 4, color = \"lightblue\") +\r\n  geom_node_text(aes(label = vapply(name, str_wrap, character(1), width = 10)), colour = \"blue\", size =4) +\r\n\r\ntheme_void()\r\n\r\n\r\n\r\nThe corpus exploration tool is used to examine the articles in two news companies. To allow a focus on a common topic, the corpus filter setting is set to include “kidnap—1”. This phrase will prompt the corporaexplorer to keep only articles with the word “kidnap” appearing at least once.\r\nA comparison between Athena Speaks – 140 article and Central Bulletin – 673 article shows that their articles’ content are almost identical with just a paraphrasing of the text, suggesting that they are likely both derivative sources of another source or a record of each other.\r\nAthena Speaks - Article 140 Central Bulletin - Article 673 \r\nThe corpus exploration tool created from the R package can also allow us to have a quick overview of the primary and derivative sources with its key term highlight function.\r\nPrimary sources include original document information such as interviews. Hence, the term “say” can be used to locate the primary source article as they represent first-hand accounts. This is placed in the red-colored term to chart and highlight. The name of others news companies can be used to locate the derivative source articles. This is placed in the blue-colored term to chart and highlight.\r\n\r\nIn the above created visualization, one can now quickly differentiate the primary and derivative sources by referring to the red-colored tiles for the primary source and blue-colored tiles for the derivative sources. For example, World Journal - Article 396 was flagged as a derivative source with its blue tile. After clicking on the tile, the document information pops up on the right and indeed, there was a reference made to “corresponding times in Abila” suggesting that World Journal is indeed a derivative source.\r\nBased on the above visualization, we can also infer that Centrum Sentinel and Homeland Illumination have primary source articles with the red document tiles.\r\nIt is helpful to understand which news source are primary sources and derivative sources as the law enforcers can derive different types of value from this two group of sources. For example, primary source are especially useful to get the latest accurate updates and to ensure minimum “embellishment” that can bring about confusion to the case. Derivative sources are especially useful for understand history of related personnel or uncover certain goals of suspects since additional research, compilation and analysis has gone into derivative sources.\r\nCodes for the corporaexplorer visualizations\r\nFor the corporaexplorer visualizations, we do not want the filters earlier applied for the correlation plots. We want to keep the news articles with their fields intact (e.g. Published: 20 January 2014) for improved analysis performance.\r\nExtracting the cleaned_text from raw_text without filtering out fields such as filter(!str_detect(text, “PUBLISHED\\:”)):\r\n\r\n\r\ncleaned_text <- raw_text %>%\r\n  group_by(newsgroup, id) %>%\r\n  filter(cumsum(text ==\"\") > 0, \r\n         cumsum(str_detect(text, \"^--\")) == 0) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nConverting the text to string format and to remove all non-word characters with a blank space:\r\n\r\n\r\ncleaned_text$text <- str_replace_all(cleaned_text$text, \"\\\\W\" ,\" \")\r\n\r\n\r\n\r\nRemove any rows in the text column with empty spaces:\r\n\r\n\r\ncleaned_text <- cleaned_text[!cleaned_text$text==\"\",]\r\ncleaned_text <- cleaned_text[!cleaned_text$text==\" \",]\r\n\r\n\r\n\r\nGrouping the cleaned_text by id and use dplyr’s summarise function to combine all rows of text in one row by id:\r\n\r\n\r\ncleaned_text <- cleaned_text %>% dplyr::group_by(id) %>%\r\n  dplyr::summarise(Text = paste(text, collapse = \" \"), .groups = \"drop_last\") \r\n\r\n\r\n\r\nUnnesting the Text column data:\r\n\r\n\r\ncleaned_text <- tidyr::unnest(cleaned_text, Text)\r\n\r\n\r\n\r\nSeparate out the id column into individual id column (e.g. id1 = “Tethys News”, id2 = “121”, id3 = “txt”):\r\n\r\n\r\ncleaned_text <- cleaned_text %>% \r\n    tidyr::separate(id, c(\"id1\", \"id2\", \"id3\"), sep=\"[-.]\")\r\n\r\n\r\n\r\nRemoving id3 as it only captures the word “txt”:\r\n\r\n\r\ncleaned_text <- subset(cleaned_text  , select = -c(id3))\r\n\r\n\r\n\r\nRenaming id1 and id2 columns to more intuitive titles:\r\n\r\n\r\nnames(cleaned_text)[names(cleaned_text) == \"id2\"] <- \"ArticleNo\"\r\nnames(cleaned_text)[names(cleaned_text) == \"id1\"] <- \"NewsGroup\"\r\n\r\n\r\n\r\nUse corporaexplorer’s explore function after setting the required definitions below:\r\n\r\n\r\ncleaned_text$for_tab_title <- paste(cleaned_text$NewsGroup, cleaned_text$ArticleNo)\r\ncorpus_revised <- prepare_data(cleaned_text, date_based_corpus = FALSE, grouping_variable = \"NewsGroup\", within_group_identifier = \"ArticleNo\", columns_doc_info = colnames(cleaned_text)[1:5], tile_length_range=c(2,2), use_matrix=FALSE)\r\nexplore(corpus_revised)\r\n\r\n\r\n\r\nQn 2: Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. Give examples.\r\nThe news sources have different biases and their bias leads them to look at the potential culprits of the crime from different perspectives, coming to different hypotheses. While bias is typically negative, in this case, it can help us examine the different possible motivations of suspects and consider the scenarios to investigate.\r\nThe focus of examination is targeted in the year 2014 as the kidnapping occurred in Jan of 2014. The data is filtered to only include news sources in 2014.\r\n\r\n\r\ncleaned_text <- dplyr::filter(cleaned_text , grepl('2014', Text))\r\n\r\n\r\n\r\nBefore diving into the biases, below is a word cloud to provide an overview of related people, places and events in 2014, the year of the kidnapping.\r\n\r\nCode for the wordcloud\r\nThe below filtering is required for a more optimal wordcloud analysis:\r\n\r\n\r\ncleaned_text <- cleaned_text %>%\r\n                    filter(!str_detect(Text, \"PUBLISHED\\\\:\")) %>%\r\n                    filter(!str_detect(Text, \"LOCATION\\\\:\")) %>%\r\n                    filter(!str_detect(Text, \"SOURCE\\\\:\")) %>%\r\n                    filter(!str_detect(Text, \"AUTHOR\\\\:\")) %>%\r\n                    filter(!str_detect(Text, \"\\\\d+:\")) %>%\r\n                    filter(!str_detect(Text, \"aandacht\")) %>%\r\n                    filter(!str_detect(Text, \"TITLE\\\\:\")) %>% \r\n                    filter(!str_detect(Text, \"continue\\\\reading\"))\r\n\r\n\r\n\r\nMost articles have subtitle words such as “published” and “title” and these are not meaningful for analysis.\r\nSuch words are not useful for analysis and are filtered out:\r\n\r\n\r\nusenet_words <- cleaned_text %>%\r\n  unnest_tokens(word, Text) %>%\r\n  filter(str_detect(word, \"[a-z']$\"), !word %in% stop_words$word, !str_detect(word, \"title\"),\r\n         !str_detect(word, \"jan\"),!str_detect(word, \"published\"), !str_detect(word, \"update\"),  \r\n         !str_detect(word, \"pm\"), !str_detect(word, \"blog\"), !str_detect(word, \"day\"), \r\n         !str_detect(word, \"morning\")\r\n  )\r\n\r\n\r\n\r\nPerform a count of words:\r\n\r\n\r\nwords_wordcloud <- usenet_words %>%\r\n  count(word, sort = TRUE) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nUse the wordcloud function for the wordcloud:\r\n\r\n\r\npal2 <- brewer.pal(8,\"Dark2\")\r\nwordcloud(words_wordcloud$word, words_wordcloud$n, min.freq=5,\r\nmax.words=150, random.order=FALSE, rot.per=.15, colors=pal2)\r\n\r\n\r\n\r\nTopic modelling of the news sources were performed to allow us to have a good overview of the bias via the topics.\r\nThe summary of the topics are as below:\r\nTopic 1\r\nTopic 2\r\nTopic 3\r\nTopic 4\r\nMissing GAStech employees jumped the city with their newfound wealth from the IPO\r\nThe kidnappers are linked to POK and APA. \r\nThe kidnappers are linked to increasingly “anarchist” POK.\r\n External people dressed in black were the suspects. There were comments that they were “lurking approximately” when the fire alarm sounded off. They are also suppliers for the breakfast meeting in the morning of the kidnapping\r\nThe heatmap diagram below shows the topic and corresponding leaning bias that each newsgroup fall into.\r\n\r\nTopic 1: The bias is positive towards the government as the keywords stone unturned here refer to the data sources justifying that the police force or government’s mistake in wrongly detaining Edvard Vann due to a confusion of identity linked to his family name.\r\n The key word Danisliau refers to the fueler Ravi Danislau who shaerd that he saw two private jets leaving the airport of Abila today carrying “business types” people.\r\n\r\nOverall, data sources in this topic seems to indicate a bias against GAStech executives escaping on their private jets with the wealth from their IPO.\r\nTopic 2: The bias is against APA. Data sources falling in this category have a number of references to APA and their related activities. A suggestion is that the kidnapping could be due to APA, Army of People of Asterian along with POK.\r\n\r\nTopic 3: Data sources falling in this category view the Kronos government and GAStech executives rather negatively as there is the key word kleptocracy suggesting corruption between the Kronos public officials and GAStech executives. They do believe that a kidnapping may have occurred as POK has become increasingly anarchist, indicating that they may have been more supportive of POK in the past before it started to get more radicalized.\r\n\r\nTopic 4: Data sources falling in this category are not biased against POK, APA or the GAStech executives and hence lean towards GAStech employees’ testimonials instead. Based on their testimonials of seeing unknown people dressed in black, they become this group of data sources’ main suspect. The coordinator of GAStech informed that these people were the suppliers/caterers for the morning’s breakfast on the day of the kidnapping and there were some suggestions to investigate these people.\r\n\r\n–\r\nCode for topic modeling\r\nRemove column that is not required from the cleaned_text dataframe:\r\n\r\n\r\ncleaned_text_lda <- subset(cleaned_text, select = -c(for_tab_title))\r\n\r\n\r\n\r\nThere were news about Singapore in the news articles but upon close examination of them in the Corporaexplorer, it was determined that they are not directly relevant to the crime investigation and the word was removed before topic modeling. There were also some spelling errors or duplicate words detected and they were removed.\r\nTokenization and removing words that are not helpful to the analysis:\r\n\r\n\r\nusenet_words_lda <- cleaned_text_lda %>%\r\n  unnest_tokens(word, Text) %>%\r\n  filter(str_detect(word, \"[a-z']$\"), !word %in% stop_words$word, !str_detect(word, \"title\"), \r\n         !str_detect(word, \"caf\"), !str_detect(word, \"singapur\"), !str_detect(word, \"jr\"), \r\n         !str_detect(word, \"singapore\"),!str_detect(word, \"threats\"), \r\n         !str_detect(word, \"international's\"), !str_detect(word, \"jan\"), !str_detect(word, \"pm\") \r\n  )\r\n\r\n\r\n\r\nPerform a word count:\r\n\r\n\r\nwords_by_newsgroup_lda <- usenet_words_lda %>%\r\n  count(NewsGroup, word, sort = TRUE) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nCreate a tf-idf:\r\n\r\n\r\ntf_idf <- words_by_newsgroup_lda %>%\r\n  bind_tf_idf(word, NewsGroup, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\n\r\n\r\nConvert the tf_idf data into a document term matrix format:\r\n\r\n\r\ntf_dtm <- tf_idf %>% cast_dtm(document = NewsGroup, term = word, value = n)\r\ntf_dtm\r\n\r\n\r\n\r\nCreate LDAVis:\r\n\r\n\r\ntopicmodels_json_ldavis <- function(fitted, doc_term){\r\n    require(LDAvis)\r\n    require(slam)\r\n\r\n    # Find required quantities\r\n    phi <- as.matrix(posterior(fitted)$terms)\r\n    theta <- as.matrix(posterior(fitted)$topics)\r\n    vocab <- colnames(phi)\r\n    term_freq <- slam::col_sums(doc_term)\r\n\r\n    # Convert to json\r\n    json_lda <- LDAvis::createJSON(phi = phi, theta = theta,\r\n                            vocab = vocab,\r\n                            doc.length = as.vector(table(doc_term$i)),\r\n                            term.frequency = term_freq)\r\n\r\n    return(json_lda)\r\n}\r\n\r\nset.seed(1234)\r\n\r\ntopic_res <- LDA(tf_dtm, 4)\r\ntf_12_json <- topicmodels_json_ldavis(\r\n  fitted = topic_res,\r\n  doc_term = tf_dtm\r\n)\r\n\r\n\r\n\r\nView the LDAvis on the server:\r\n\r\n\r\nserVis(tf_12_json)\r\n\r\n\r\n\r\nApplying the newsgroups back to the topic distribution:\r\n\r\n\r\nlda.model <- topicmodels::LDA(tf_dtm, 4, method = \"Gibbs\", control = list(iter=2000, seed = 1234))\r\ntheta <- as.data.frame(topicmodels::posterior(lda.model)$topics)\r\ntheta\r\n\r\n\r\n\r\nConverting the theta results into a tibble format:\r\n\r\n\r\ntheta_tidy <- \r\n    theta %>% \r\n    as_tibble(rownames=\"Newsgroup\") \r\n\r\ntheta_tidy\r\n\r\n\r\n\r\nPivoting long to change the tibble format into heatmap function’s required format with TidyR:\r\n\r\n\r\ntheta_tidy <- theta_tidy %>%\r\n tidyr::pivot_longer(\r\n     cols = starts_with(\"Topic\"), \r\n     names_to = \"Topic_no\", \r\n     values_to = \"result\", \r\n     names_prefix = \"Topic_\")\r\n\r\n\r\n\r\nApply the heatmap function and visualize the heatmap:\r\n\r\n\r\ntheta_tidy_heatmap <- \r\n    theta_tidy %>%\r\n    heatmap(Newsgroup, Topic_no, result, \r\n            rect_gp = grid::gpar(lwd = 0.5), col= colorRampPalette(brewer.pal(8, \"Blues\"))(25)) \r\n\r\ntheta_tidy_heatmap\r\n\r\n\r\n\r\nQn 3: Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships.\r\nPersonal relationships among GAStech employees\r\nNon-work related emails sent between colleagues at GAStech can help us uncover personal relationships among the GAStech employees as the frequency of such emails suggest a closer personal relationship.\r\n\r\nThere are two pairs of relationships in the above diagram that are particularly striking.\r\nOne is between Rachel Pantanal (id: 38), Assistant to CIO and Isia Vann (id: 43), Perimeter Control. They display a close relationship as we can see that they exchange non-work related emails with each other on majority of the days in the week (Monday, Tuesday, Wednesday, Friday). On closer inspection of their emails, there was an email about whether Rachel liked the flowers and she responded positively. This suggests a potential romantic relationship between the both of them. Isia Vann has a personal grudge against GAStech for the death of his sister and is part of the more radical-minded members at POK according to the historical documents. If he is in a romantic relationship with Rachel, Rachel may be sympathetic to his causes.\r\n\r\nAnother pair is Rachel Pantanal (id: 38) and Ruscella Mies Haber (id: 33) as they shared a non-work related email exchange on Sunday. It is very unusual for employees at GAStech to send non-work related emails to each other on weekends so this is an outlier. Upon inspection of the data, it was revealed that the exchanged email was titled “RE: FW: ARISE - Inspiration for Defenders of Kronos”.\r\n\r\nARISE is a publication by the Asterian People’s Army APA , a paramilitary organization which has been engaged in terrorist activities funded through its criminal enterprises which include drug trafficking and has been associated with POK. Please refer to the data table diagram below.\r\n The fact that the email was sent on Sunday might also be due to some level of urgency prior to the kidnapping.\r\nExchange of the suspicious “ARISE - Inspiration for Defenders of Kronos” email\r\n\r\n\r\nThe visNetwork package was used for the interactive visNetwork diagram. There is a manipulation feature that allows us to push the nodes further apart so that they do not look so cluttered and obscure readers’ readability. When this feature is set to “True”, an accompanying “edit” icon appears on the diagram to indicate that this feature is now active for the diagram.\r\nThe suspicious “ARISE” email was exchanged among the above group of colleagues. It was first sent from Rachel to Ruscella and then there was an exchange of information among Hennie, Ruscella, Loreto, Isia, Inga and Minke, suggesting an ongoing conversation about this email among the group.\r\nAs Rachel is from the administration department, investigators may be able to find out more clues from other members of the administration department as there is frequent interaction between Rachel and her colleagues in the same department on non-work related emails suggesting a close relationship with them.\r\nThe filter for this diagram is non-work related after office hours emails.\r\n\r\nIsia Vann has non-work related email exchanges with Rachel Pantanal (Assistant to CIO, Tethys Citizenship), Claudio Hawelon (Truck Driver, Kronos Citizenship), Mat Bramar (Assistant to CEO, Tethys Citizenship) and Inga Ferro (Site Control, Kronos Citizenship). Although Claudio did not receive the “ARISE” email, police investigators might still want to try to interview him since he seems to have a relatively closer relationship with Isia than other colleagues in the company especially since there were emails exchanges on non-work related subject after office work hours.\r\nThe filter for this diagram is non-work related after office hours emails.\r\n\r\nClusters of relationships by department\r\nThe image below shows the email exchanges for non-work-related emails outside office hours with at least 2 exchanges in the 14-day period. There are four clusters with such exchanges. One cluster is the facilities department among their own department members. Another cluster is the IT technicians without the IT manager. For the executives cluster, the cluster is among the CEO, CIO and Environmental Safety Advisor.\r\n\r\nThe fourth cluster is the administration cluster connected with some members of the security department through Rachel and Isia. This again highlights an anomaly as colleagues usually form closer informal relationships within their department and less often outside their department. The relationship between Rachel and Isia is worthy to be further investigated.\r\nIt is also interesting to note that the news articles mentioned that Edvard Vann, the guard of safety at GAStech was questioned for hours on suspicion of his involvement with the kidnapping due to the similarity between his name and that of a Protector of Kronos member. However,in the above diagram, it is revealed that Edvard is not only not connected to the suspected group of people with the suspicious “Arise” email, he also does not correspond with any of the other colleagues outside office hours, suggesting that he does not have a strong personal relationship with his colleagues. Unless he uses other modes of communication, this does suggest that he may indeed not be related to the suspected “Protector of Kronos” member.\r\nExamining both employment period and military service period to identify relationships\r\n\r\n\r\nThe patchwork package was used for placing multiple plots together in the same figure.\r\nIn the above visualization, the data is filtered to spotlight the suspicious employees who exchanged the suspicious “ARISE” email. From the visualization, it can be seen that four out of seven of the suspicious GAStech employees were relatively new in the company, having joined less than a year since the kidnapping date. Among all the employees at GAStech, Rachel Pantanal has the shortest tenure and may potentially have entered GAStech with an agenda to help Isia Vann with his “revenge” goal. It is also possible that these newly recruited employees are part of the increasingly “radicalised” POK members that infiltrated into GAStech. They also took on roles that allow them intimate access to the executives’ whereabouts (e.g. Rachel Pantanal is the assistant to CIO and the rest besides Ruscella are all in the security department).\r\nThe below visualization shows the employment period of all employees in GAStech and most of the employees have been at GAStech for many years and the suspicious employees are from a very small minority of employees who joined GAStech less than 3 years.\r\n\r\nRelationships with the government (military)\r\nAn honorable discharge occurs when a military service member received a good or excellent rating for their service time, by exceeding standards for performance and personal conduct. If a service member’s performance is satisfactory but the individual failed to meet all expectations of conduct for military members, the discharge is considered a General Discharge. To receive a General Discharge from the military, there has to be some form of nonjudicial punishment to correct unacceptable military behavior or failure to meet military standards1.\r\n\r\nIt is worthy to investigate why the majority of the suspected group was given a general discharge and if it was due to extreme ideologies/goals. With the exception of Ruscella as she was much older, all of the suspected members in the group (Hennie, Isia, Loreto, Inga, Minke) had overlaps in service with at least one other member in this group. This means that there were likely some unofficial relationships/contact with each other in the Army before they joined GAStech. The “General Discharge” instead of “Honorable Discharge” could also potentially be a reflection of their relationships with the Krono government (i.e. concerns that Krono government has flagged out regarding them).\r\nAs Rachel is a Tethys citizen, she did not serve in the Armed Forces of Kronos and will not be reflected in this diagram.\r\nRelationships between APA, POK and GAStech\r\nThe visualization below maps the potential official and unofficial relationships that the GAStech employees potentially have with APA, POK.\r\n\r\n\r\nThe ggalluvial package is a ggplot2 extension for producing alluvial plots in a tidyverse framework. This type of visualization is especially useful for categorical data. It is used to show the relationship flows in the visualization here.\r\nIt has come to our attention that four out of six members of the earlier mentioned suspected group have relationships with APA and POK through connected family members, assuming that the shared family name does indeed indicate that they are family and not a mere coincidence. Connected family members may share some common ideologies and affiliations and this point needs to be further investigated.\r\nWhen we overlap the multiple visualizations above, they point towards the suspected group of GAStech employees as they do have a common goal through their multiple connections especially with POK and/or APA. This scenario likely has a the highest likelihood among all the scenarios.\r\nHowever, there is also the other possibility that the four GAStech executives that were claimed to be missing/kidnapped were actually on a personal impromptu golf vacation to celebrate their windfall from the IPO. The diagram below shows the email exchange with subject header containing “vacation”, suggesting that the executives were planning for a vacation in the very recent period before the supposed “kidnapping”. Note that the email headers provided for analysis were two weeks of emails prior to the “kidnapping” incident. The executives might have simply gone for a vacation to celebrate and were not kidnapped at all.\r\n\r\nThe four of them also matches the development of the police correcting the number of missing GAStech employees from fourteen to ten. POK might have issued a ransom note to take advantage of the situation but may or may not be actually involved in the kidnapping despite their claims.\r\nIt is also possible that the suppliers dressed in black have kidnapped the other ten employees in order to obtain a ransom from Sten Sanjorge Jr, CEO of GAStech who is now a billionare after the company’s IPO. They may not have been fully aware of the executives’ vacation plans and hence did not succeed in kidnapping them but simply captured ten other employees during their planned time window with the chaos caused by the false fire alarm. The affiliation of the suppliers is not clear but it is likely that they managed to get into the GAStech building with the help of an GAStech administrative executive who have contracted them to cater for the reunion breakfast between the Kronos government and GAStech executives.\r\nThe police investigators are advised to investigate the above potential suspicious scenarios and look into the aforementioned possible suspects.\r\n–\r\nCodes for visualizations in Question 3\r\nRaw text needs to be first encoded to UTF8 format before use in the data table.\r\nTo create data table of raw text:\r\n\r\n\r\nraw_text_utf8 <- raw_text_lda %>% mutate_if(is.character, utf8_encode)\r\n\r\nDT::datatable(raw_text_utf8, filter = 'top') \r\n\r\n\r\n\r\nCreate galluvial diagram:\r\n\r\n\r\nrelated_table <- tibble(\r\n  Suspects = c(\"Loreto Bodrogi\", \"Loreto Bodrogi\", \"Varro Awelon\", \"Hennie Osvaldo\", \r\n               \"Isia Vann\", \"Isia Vann\", \"Minke Mies\", \"Minke Mies\", \"Varro Awelon\", \r\n               \"Hennie Osvaldo\", \"Loreto Bodrogi\"),\r\n  Org = c(\"Gastech\", \"Gastech\", \"Gastech\", \"Gastech\", \"Gastech\", \"POK\", \r\n          \"Gastech\", \"POK\", \"APA\", \"POK\", \"APA\"),\r\n  Connections = c(\"Carmin Bodrogi\" , \"Henk Bodrogi\", \"Cynthe Awelon\", \"Carmine Osvaldo\", \r\n                  \"Juliana Vann\", \"Juliana Vann\", \"Valentine Mies\", \"Valentine Mies\", \"Cynthe Awelon\",\r\n                  \"Carmine Osvaldo\", \"Carmin Bodrogi\"),\r\n  n = c( 1, 1, 1, 1,1, 1,1, 1,1, 1,1)\r\n)\r\n\r\nggplot(related_table,\r\n       aes(axis1 = Suspects,\r\n           axis2 = Org,\r\n           axis3 = Connections\r\n           )) +\r\n  geom_alluvium(aes(fill = Suspects)) +\r\n  geom_stratum() +\r\n  geom_text(stat = \"stratum\", \r\n            aes(label = after_stat(stratum))) +\r\n  scale_x_discrete(limits = c(\"Suspects\", \"Org\", \"Connections\"),\r\n                   expand = c(.1, .1)) +\r\n  scale_fill_viridis_d() +\r\n  labs(title = \"Relationships with APA, Gastech and POK\",\r\n       subtitle = \"stratified by Suspects, Org, and Connections\"\r\n       ) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\", axis.text.y = element_blank(),axis.ticks = element_blank() ) \r\n\r\n\r\n\r\nReading the email headers data:\r\n\r\n\r\nemail_headers <- read_csv(\"email headers.csv\")\r\n\r\n\r\n\r\nIn the email headers, the reply emails typically start with “RE”. As we want unique email headers for labelling into work and non-work related emails, we will filter them out.\r\nGetting unique email titles for manual labelling:\r\n\r\n\r\nemail_headers <- email_headers %>%\r\n  filter(!str_detect(Subject, \"RE: \"))\r\n\r\n\r\n\r\nRetain only subject header column for purpose of categorization:\r\n\r\n\r\nemail_headers <- subset(email_headers , select = -c(From, To, Date))\r\n\r\n\r\n\r\nRepetitve email headers are grouped together and a count is performed:\r\n\r\n\r\nemail_headers_count <- email_headers %>%\r\n        count(Subject, sort = TRUE)\r\n\r\n\r\n\r\nWrite to csv to perform manual labelling of 154 observations:\r\n\r\n\r\n#write_csv(email_headers_count, \"email_headers_count.csv\", append = FALSE)\r\n\r\n\r\n\r\nRead csv of manually labelled 154 observations:\r\n\r\n\r\nemail_headers_count <- read_csv(\"email_headers_count.csv\")\r\n\r\n\r\n\r\nUse dyplr full join to join back with email_headers table with 315 observations:\r\n\r\n\r\nemail_headers <- email_headers %>% full_join(email_headers_count, by = \"Subject\")\r\n\r\n\r\n\r\nRemove the count column:\r\n\r\n\r\nemail_headers <- subset(email_headers , select = -c(n))\r\n\r\n\r\n\r\nThe str_c function joins ombines multiple character vectors into a single character vector.\r\nConvert data to string:\r\n\r\n\r\nemail_headers_re <- email_headers %>%\r\n  mutate(str_c(\"RE: \", Subject)) \r\n\r\n\r\n\r\nRename the new column:\r\n\r\n\r\nnames(email_headers_re)[names(email_headers_re) == \"str_c(\\\"RE: \\\", Subject)\"] <- \"Subject\"\r\n\r\n\r\n\r\nAdd the subject type column back to original email headers table:\r\n\r\n\r\nemail_headers_ori <- read_csv(\"email headers.csv\")\r\ncolnames(email_headers_ori)\r\ncolnames(email_headers)\r\nemail_headers_ori <- email_headers_ori %>% left_join(email_headers, by = \"Subject\")\r\nemail_headers_ori <- distinct(email_headers_ori, across())\r\n\r\n\r\n\r\nEnsure the left join is performed on distinct entries:\r\n\r\n\r\nemail_headers_ori_csv <- read_csv(\"email headers.csv\")\r\nemail_headers_ori_re <- dplyr::left_join(email_headers_ori_csv, email_headers_re)\r\nemail_headers_ori_re <- email_headers_ori_re %>% distinct()\r\n\r\n\r\n\r\nPerform a left join by “Subject” column:\r\n\r\n\r\nemail_headers_ori_re <- dplyr::left_join(email_headers_ori_re, email_headers, by=\"Subject\")\r\n\r\n\r\n\r\nUnite the subject type after left join:\r\n\r\n\r\nemail_headers_ori_re <- email_headers_ori_re %>% \r\n  unite('Subject_Type', `Subject type.x`:`Subject type.y`, remove = TRUE)\r\n\r\n\r\n\r\nThere are some email headers that did not have the no “RE” and “RE” pairings. Such email headers form only a very small group and we will analyse them to determine whether they should be classified as work-related or non-work related.\r\nReview subject types that have not been coded:\r\n\r\n\r\nemail_headers_ori_re_NA <- filter(email_headers_ori_re, Subject_Type == \"NA_NA\")\r\n\r\n\r\n\r\nAfter reviewing them, these email headers are assessed to be non-work related email types.\r\nRename the subject type to “Non-work related”:\r\n\r\n\r\nemail_headers_ori_re <- email_headers_ori_re  %>% \r\n  mutate_at(\"Subject_Type\", str_replace, \"NA_NA\", \"Non-work related\") \r\n\r\nemail_headers_ori_re <- email_headers_ori_re  %>% \r\n  mutate_at(\"Subject_Type\", str_replace, \"NA_\", \"\") %>% \r\n  mutate_at(\"Subject_Type\", str_replace, \"_NA\", \"\") \r\n\r\n\r\n\r\nWrangling with date data using the ‘anytime’ package released in 2020 - extract day of week from sent date:\r\n\r\n\r\nemail_headers_ori_re$SentDate <- anytime(email_headers_ori_re$Date)\r\nemail_headers_ori_re$SentDate <- iso8601(anydate(email_headers_ori_re$SentDate))\r\nemail_headers_ori_re$Weekday = wday(email_headers_ori_re$SentDate)\r\n\r\n\r\n\r\nGeneral office hours are assumed to be between 7am - 6:59pm. Drivers might be on shift but generally, it may be unusual to be sending emails outside regular office hours and requires further investigation.\r\nCategorize sent hour to during or outside work hours:\r\n\r\n\r\n#wrangling time - using the 'anytime' package released in 2020\r\nemail_headers_ori_re$SentTime <- anytime(email_headers_ori_re$Date)\r\nemail_headers_ori_re$SentHour <- hour(email_headers_ori_re$SentTime)\r\nemail_headers_ori_re <- email_headers_ori_re  %>% \r\n  mutate_at(\"SentHour\", str_replace_all, c(\"19\" = \"Outside_work_hours\", \"20\" = \"Outside_work_hours\", \r\n                                           \"21\" = \"Outside_work_hours\", \"22\" = \"Outside_work_hours\", \r\n                                           \"18\" = \"During_work_hours\", \"17\" = \"During_work_hours\", \r\n                                           \"16\" = \"During_work_hours\",\"15\" = \"During_work_hours\", \r\n                                           \"14\" = \"During_work_hours\", \"13\" = \"During_work_hours\", \r\n                                           \"12\" = \"During_work_hours\", \"11\" = \"During_work_hours\", \r\n                                           \"10\" = \"During_work_hours\", \"9\" = \"During_work_hours\", \r\n                                           \"8\" = \"During_work_hours\", \"7\" = \"During_work_hours\", \r\n                                           \"6\" = \"Outside_work_hours\", \"5\" = \"Outside_work_hours\",\r\n                                           \"4\" = \"Outside_work_hours\", \"3\" = \"Outside_work_hours\",\r\n                                           \"2\" = \"Outside_work_hours\", \"1\" = \"Outside_work_hours\",\r\n                                           \"0\" = \"Outside_work_hours\"))\r\n\r\nemail_headers_ori_re <- email_headers_ori_re  %>% \r\n  mutate_at(\"Weekday\", str_replace_all, \r\n            c(\"7\" = \"Sunday\", \"6\" = \"Saturday\", \"5\" = \"Friday\", \r\n              \"4\" = \"Thursday\", \"3\" = \"Wednesday\", \"2\" = \"Tuesday\", \"1\" = \"Monday\"))\r\n\r\n\r\n\r\nPrepare the processed email headers data after tidying the data:\r\n\r\n\r\nsplit_to_email_headers <- separate_rows(email_headers_ori_re, To, sep = \",\")\r\nprocessed_email_headers <- split_to_email_headers %>% \r\n    tidyr::separate(From, c(\"Sender_First\", \"Sender_LastName\")) %>%\r\n    tidyr::unite('Sender_fullname',c(\"Sender_First\", \"Sender_LastName\"), sep=\" \") %>%\r\n    tidyr::separate(To, c(\"Recipient_First\", \"Recipient_LastName\"), sep=\"[,.@]\") %>%\r\n    tidyr::unite('Recipient_fullname',c(\"Recipient_First\", \"Recipient_LastName\"), sep=\" \")\r\n\r\n\r\n\r\nAdjust for words in names for 4 employees with “-”, >1 word etc as the earlier separation rule did not work as well for the more unusual name formats:\r\n\r\n\r\nprocessed_email_headers <- processed_email_headers %>% \r\n  mutate_at(\"Sender_fullname\", str_replace, \"Campo\", \"Campo-Corrente\") %>% \r\n  mutate_at(\"Sender_fullname\", str_replace, \"Sanjorge\", \"Sanjorge Jr.\") %>% \r\n  mutate_at(\"Sender_fullname\", str_replace, \"Vasco\", \"Vasco-Pais\") %>% \r\n  mutate_at(\"Sender_fullname\", str_replace, \"Ruscella Mies\", \"Ruscella Mies Haber\") %>%\r\n  mutate_at(\"Recipient_fullname\", str_replace, \"Ruscella Mies\", \"Ruscella Mies Haber\") %>% \r\n  mutate_at(\"Recipient_fullname\", str_replace, \"Jr\", \"Jr.\") \r\n\r\n\r\n\r\nRemove leading white space:\r\n\r\n\r\nprocessed_email_headers$Recipient_fullname <- \r\n  trimws(processed_email_headers$Recipient_fullname, \"l\")\r\n\r\n\r\n\r\nFilter out cases where sender sends to him/herself (likely in cc), reduces to 8170 obs:\r\n\r\n\r\nprocessed_email_headers <- \r\nprocessed_email_headers[processed_email_headers$Sender_fullname!=processed_email_headers$Recipient_fullname,]\r\n\r\n\r\n\r\nSometimes, after reviewing the data output, one may realise that the classification is incorrect. The below code chunk illustrates what to do when an email type has been misclassified.\r\nChanging the classification of email subject types:\r\n\r\n\r\nprocessed_email_headers <- processed_email_headers %>% \r\n    tidyr::unite('Merged',c(\"Subject\", \"Subject_Type\"), sep=\"_\")\r\n\r\nprocessed_email_headers <- processed_email_headers %>% \r\n  mutate_at(\"Merged\", str_replace, \"Upcoming birthdays_Non-work related\", \"Upcoming birthdays_Work related\") %>%\r\n  mutate_at(\"Merged\", str_replace, \"RE: Upcoming birthdays_Non-work related\", \r\n            \"RE: Upcoming birthdays_Work related\") %>%\r\n  mutate_at(\"Merged\", str_replace, \"FW: ARISE - Inspiration for Defenders of Kronos_Work related\", \r\n            \"FW: ARISE - Inspiration for Defenders of Kronos_Non-work related\") %>% \r\n  mutate_at(\"Merged\", str_replace, \"RE: FW: ARISE - Inspiration for Defenders of Kronos_Work related\", \r\n            \"RE: FW: ARISE - Inspiration for Defenders of Kronos_Non-work related\")\r\n\r\nprocessed_email_headers <- processed_email_headers %>% \r\n    tidyr::separate(Merged, c(\"Subject\", \"Subject_Type\"), sep=\"_\", remove = TRUE)\r\n\r\n\r\n\r\nChanging the classification of sent period:\r\n\r\n\r\nprocessed_email_headers <- processed_email_headers %>% \r\n    tidyr::unite('Merged',c(\"Weekday\", \"SentHour\"), sep=\"-\")\r\n\r\nprocessed_email_headers <- processed_email_headers  %>% \r\n  mutate_at(\"Merged\", str_replace, \"Sunday-During_work_hours\", \"Sunday-Outside_work_hours\")\r\n\r\nprocessed_email_headers <- processed_email_headers %>% \r\n    tidyr::separate(Merged, c(\"Weekday\", \"SentHour\"), sep=\"-\", remove = TRUE)\r\n\r\n\r\n\r\nDefining sources and destinations for network diagram:\r\n\r\n\r\nsources <- processed_email_headers %>%\r\n  distinct(Sender_fullname) %>%\r\n  rename(label = Sender_fullname)\r\n\r\ndestinations <- processed_email_headers %>%\r\n  distinct(Recipient_fullname) %>%\r\n  rename(label = Recipient_fullname)\r\n\r\n\r\n\r\nRead in employee records data that will be used to prepare nodes:\r\n\r\n\r\nemployee_records <- read_excel(\"EmployeeRecords.xlsx\", sheet = \"Employee Records\")\r\nemployee_records  <- subset(employee_records, \r\n                            select = -c(BirthDate, BirthCountry, Gender, \r\n                                        CitizenshipBasis, CitizenshipStartDate, \r\n                                        PassportCountry, PassportIssueDate, \r\n                                        PassportExpirationDate, CurrentEmploymentStartDate, \r\n                                         EmailAddress, MilitaryDischargeDate ))\r\n\r\n\r\n\r\nProcess employee records to combine first and last name to full name:\r\n\r\n\r\nemployee_records <- employee_records %>% \r\n    tidyr::unite('Employee_Name',c(\"FirstName\", \"LastName\"), sep=\" \", remove = TRUE)\r\n\r\n\r\n\r\nRename “CurrentEmploymentType” to the more intuitive title, “Department”:\r\n\r\n\r\nnames(employee_records)[names(employee_records) == \"CurrentEmploymentType\"] <- \"Department\"\r\n\r\n\r\n\r\nThere are 54 employees and there should be 54 rows.\r\nCreate and review the nodes:\r\n\r\n\r\nnodes <- full_join(sources, destinations, by = \"label\")\r\nnodes <- nodes %>% rowid_to_column(\"id\")\r\nnodes <- nodes %>% left_join(employee_records, by = c(\"label\"=\"Employee_Name\"))\r\nnodes <- distinct(nodes, across())\r\n\r\nnodes\r\n\r\n\r\n\r\nPrepare the route:\r\n\r\n\r\nper_route <- processed_email_headers %>%  \r\n  filter(Subject_Type == \"Non-work related\") %>%\r\n  group_by(Sender_fullname, Recipient_fullname, Weekday) %>%\r\n  summarise(weight = n()) %>% \r\n  filter(weight > 0) %>%\r\n  ungroup()\r\nper_route\r\n\r\n\r\n\r\nThe grepl function allows us to specify specific key words in the subject header that we want to filter out for analysis.\r\nHere, the example shown is to filter out “ARISE” email but note that we repeat this for other specific emails such as “vacation”.\r\nA sample of modifying the route to filter out targeted email conversations:\r\n\r\n\r\nper_route_v2 <- \r\n dplyr::filter(processed_email_headers , grepl(\"ARISE\", Subject)) %>%\r\n               \r\n  group_by(Sender_fullname, Recipient_fullname, Weekday) %>%\r\n  summarise(weight = n()) %>% \r\n  filter(weight > 0) %>%\r\n  ungroup()\r\nper_route_v2\r\n\r\n\r\n\r\nCreating the edges for the network diagram:\r\n\r\n\r\nedges <- per_route %>% \r\n  left_join(nodes, by = c(\"Sender_fullname\" = \"label\")) %>% \r\n  rename(from = id)\r\n\r\nedges <- edges %>% \r\n  left_join(nodes, by = c(\"Recipient_fullname\" = \"label\")) %>% \r\n  rename(to = id)\r\n\r\nedges\r\n\r\n\r\n\r\nCreating the tibble graph:\r\n\r\n\r\nGAStech_graph <- tbl_graph(nodes = nodes, edges = edges, directed = TRUE)\r\nGAStech_graph\r\n\r\n\r\n\r\nActivating the edges:\r\n\r\n\r\nGAStech_graph %>%\r\n  activate(edges) %>%\r\n  arrange(desc(weight))\r\n\r\n\r\n\r\nCreating facet graphs for non-work related emails on a weekday by department:\r\n\r\n\r\nset.seed(123)\r\n\r\nset_graph_style()\r\n\r\ng <- ggraph(GAStech_graph, layout = \"fr\") + geom_edge_link(aes(width=weight), alpha=0.2) +\r\n     scale_edge_width(range = c(0.1,5)) + geom_node_point(aes(color = Department), size = 3) +\r\n     theme(text=element_text(family=\"Arial\", size=16)) +  geom_node_text(aes(label = id)) \r\n\r\ng + facet_edges(~Weekday) + theme(legend.position = \"bottom\") +\r\n    ggtitle(\"Non-work related emails exchanged\")\r\n\r\n\r\n\r\nPrepare nodes for visNetwork:\r\n\r\n\r\nnodes_dep <- nodes %>%\r\n  rename(group = Department)\r\n\r\n\r\n\r\nApply visNetwork function:\r\n\r\n\r\nvisNetwork(nodes_dep, edges) %>%\r\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\r\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = TRUE) %>%\r\n  visLegend() %>%\r\n  visLayout(randomSeed = 123)\r\n\r\n\r\n\r\nCreate data table for nodes for easy identification of ids to full names and other details:\r\n\r\n\r\nDT::datatable(nodes, filter = 'top') %>%\r\n   formatStyle(0, target = 'row', lineHeight='75%')\r\n\r\n\r\n\r\nData processing for analyzing military service periods:\r\n\r\n\r\nemployee_records <- read_excel(\"EmployeeRecords.xlsx\", sheet = \"Employee Records\")\r\nemployee_records  <- employee_records %>% filter(!str_detect(MilitaryDischargeType, \" \"))\r\nemployee_records <- subset(employee_records , select = -c(EmailAddress))\r\nglimpse(employee_records)\r\n\r\n\r\n\r\n\r\n\r\nemployee_records <- employee_records %>% \r\n    tidyr::unite('Fullname',c(\"FirstName\", \"LastName\"), sep=\" \")\r\nglimpse(employee_records)\r\n\r\n\r\n\r\n\r\n\r\nemployee_records$BirthDate <-  ymd(employee_records$BirthDate)\r\nglimpse(employee_records)\r\n\r\n\r\n\r\n\r\n\r\nemployee_records$MilitaryDischargeDate <-  ymd(employee_records$MilitaryDischargeDate)\r\n\r\n\r\n\r\n\r\n\r\nemployee_records_Kronos <- employee_records %>% filter(CitizenshipCountry == \"Kronos\")\r\n\r\nglimpse(employee_records_Kronos) \r\n\r\n\r\n\r\nBased on the factbook, all Krono citizens need to serve the military at age 18. Hence, we define the militart start date with the formula of birthdate + 18 years.\r\n\r\n\r\nemployee_records_Kronos$MilitaryStartDate <- employee_records_Kronos$BirthDate %m+% years(18) \r\nglimpse(employee_records_Kronos) \r\n\r\n\r\n\r\n\r\n\r\nemployee_records_Kronos <- employee_records_Kronos %>% \r\n    tidyr::unite('Branch_and_Discharge',c(\"MilitaryServiceBranch\", \"MilitaryDischargeType\"), sep=\" - \")\r\nglimpse(employee_records_Kronos) \r\n\r\n\r\n\r\nAdd a Column to a Dataframe Based on Other Column with dplyr - flagging employees with the suspicious ARISE email:\r\n\r\n\r\nemployee_records_Kronos <- employee_records_Kronos %>%\r\n  mutate(Status = case_when(\r\n    str_detect(Fullname, \"Rachel Pantanal\") ~ \"Flagged\",\r\n    str_detect(Fullname, \"Ruscella Mies Haber\") ~ \"Flagged\",\r\n    str_detect(Fullname, \"Hennie Osvaldo\") ~ \"Flagged\",\r\n    str_detect(Fullname, \"Isia Vann\") ~ \"Flagged\",\r\n    str_detect(Fullname, \"Loreto Bodrogi\") ~ \"Flagged\",\r\n    str_detect(Fullname, \"Inga Ferro\") ~ \"Flagged\",\r\n    str_detect(Fullname, \"Minke Mies\") ~ \"Flagged\",\r\n    TRUE ~ \"Not flagged\"\r\n    ))\r\n\r\n\r\n\r\nA filter is added to isolate employees who received the suspicious “ARISE” email. However, this step is repeated without the filter to obtain military service periods for all Krono citizen employee citizens for one of the visualization.\r\n\r\n\r\nemployee_records_Kronos <- subset(employee_records_Kronos , select = -c(BirthDate, BirthCountry, Gender, CitizenshipCountry, CitizenshipBasis, CitizenshipStartDate, PassportCountry, PassportIssueDate, PassportExpirationDate, CurrentEmploymentType, CurrentEmploymentTitle,CurrentEmploymentStartDate ))\r\n\r\n\r\nemployee_records_Kronos <- employee_records_Kronos %>% filter(Status == \"Flagged\")\r\nemployee_records_Kronos <- subset(employee_records_Kronos,  select = -c(Status))\r\nemployee_records_Kronos\r\n\r\n\r\n\r\n\r\n\r\nemployee_records_Kronos.long <- employee_records_Kronos %>%\r\n  mutate(Start = ymd(MilitaryStartDate),\r\n         End = ymd(MilitaryDischargeDate)) %>%\r\n  gather(date.type, employee_records_Kronos.date, -c(Branch_and_Discharge, Fullname)) %>%\r\n  arrange(date.type, employee_records_Kronos.date) %>%\r\n  mutate(Fullname = factor(Fullname, levels=rev(unique(Fullname)), ordered=TRUE))\r\n\r\n\r\n\r\n\r\n\r\ntheme_update(plot.title = element_text(hjust = 0.5))\r\ntheme_gantt <- function(base_size=11, base_family=\"Source Sans Pro Light\") {\r\n  ret <- theme_bw(base_size, base_family) %+replace%\r\n    theme(panel.background = element_rect(fill=\"#ffffff\", colour=NA),\r\n          axis.title.x=element_text(vjust=-0.2), axis.title.y=element_text(vjust=1.5),\r\n          title=element_text(vjust=1.2, family=\"Source Sans Pro Semibold\"),\r\n          panel.border = element_blank(), axis.line=element_blank(),\r\n          panel.grid.minor=element_blank(),\r\n          panel.grid.major.y = element_blank(),\r\n          panel.grid.major.x = element_line(size=0.5, colour=\"grey80\"),\r\n          axis.ticks=element_blank(),\r\n          legend.position=\"bottom\", \r\n          axis.title=element_text(size=rel(0.8), family=\"Source Sans Pro Semibold\"),\r\n          strip.text=element_text(size=rel(1), family=\"Source Sans Pro Semibold\"),\r\n          strip.background=element_rect(fill=\"#ffffff\", colour=NA),\r\n          panel.spacing.y=unit(1.5, \"lines\"),\r\n          legend.key = element_blank())\r\n  \r\n  ret\r\n}\r\n\r\n# Calculate where to put the dotted lines that show up every three entries\r\nx.breaks <- seq(length(employee_records_Kronos$Fullname) + 0.5 - 3, 0, by=-3)\r\n\r\n# Build plot\r\ntimeline_Kronosmilitary <- ggplot(employee_records_Kronos.long, \r\n                                  aes(x=Fullname, y=employee_records_Kronos.date, colour=Branch_and_Discharge)) + \r\n                                  geom_line(size=6) + \r\n                                  geom_vline(xintercept=x.breaks, colour=\"grey80\", linetype=\"dotted\") + \r\n                                  guides(colour=guide_legend(title=NULL)) +\r\n                                  labs(x=NULL, y=NULL) + coord_flip() +\r\n                                  scale_y_date(date_breaks=\"2 years\", date_labels = (\"%Y\")) +\r\n                                  theme_economist() + scale_colour_economist() + \r\n                                  theme(axis.text.x=element_text(angle=0, vjust = 1, size = 12, face = 'bold'), \r\n                                        axis.text.y = element_text(size = 12, face = 'bold'), \r\n                                        legend.text = element_text(size=12)) +\r\n                                  ggtitle(\"Military Service Dates\") + \r\n                                  theme(plot.title = element_text(hjust = 0.5, size = 18, color = \"darkblue\"))\r\n\r\ntimeline_Kronosmilitary\r\n\r\n\r\n\r\nRepeating the steps to create gantt chart for employee’s employment start date. The kidnapping start date is set on Jan 2014 to allow us to see the service period from employment start date to the “suspected kidnapping” date.\r\n\r\n\r\nemployee_records_startdate <- subset(employee_records_startdate , select = -c(BirthDate, BirthCountry, Gender, CitizenshipCountry, CitizenshipBasis, CitizenshipStartDate, PassportCountry, PassportIssueDate, PassportExpirationDate, CurrentEmploymentTitle, EmailAddress, MilitaryDischargeDate, MilitaryDischargeType, MilitaryServiceBranch))\r\n\r\nemployee_records_startdate$KidnappingStartDate <- ymd('2014-01-20')\r\nemployee_records_startdate$CurrentEmploymentStartDate <- ymd(employee_records_startdate$CurrentEmploymentStartDate)\r\n\r\n#filter is added to improve focus on employees who are flagged\r\nemployee_records_startdate <- employee_records_startdate %>% filter(Status == \"Flagged\")\r\nemployee_records_startdate <- subset(employee_records_startdate,  select = -c(Status))\r\nemployee_records_startdate\r\n\r\n\r\n\r\n\r\n\r\nemployee_records_startdate.long <- employee_records_startdate %>%\r\n  mutate(Start = ymd(CurrentEmploymentStartDate),\r\n         End = ymd(KidnappingStartDate)) %>%\r\n  gather(date.type, employee_records_startdate.date, -c(CurrentEmploymentType, Fullname)) %>%\r\n  arrange(date.type, employee_records_startdate.date) %>%\r\n  mutate(Fullname = factor(Fullname, levels=rev(unique(Fullname)), ordered=TRUE))\r\n\r\n\r\n\r\n\r\n\r\n# Custom theme for making a clean Gantt chart\r\ntheme_update(plot.title = element_text(hjust = 0.5))\r\ntheme_gantt <- function(base_size=11, base_family=\"Source Sans Pro Light\") {\r\n  ret <- theme_bw(base_size, base_family) %+replace%\r\n    theme(panel.background = element_rect(fill=\"#ffffff\", colour=NA),\r\n          axis.title.x=element_text(vjust=-0.2), axis.title.y=element_text(vjust=1.5),\r\n          title=element_text(vjust=1.2, family=\"Source Sans Pro Semibold\"),\r\n          panel.border = element_blank(), axis.line=element_blank(),\r\n          panel.grid.minor=element_blank(),\r\n          panel.grid.major.y = element_blank(),\r\n          panel.grid.major.x = element_line(size=0.5, colour=\"grey80\"),\r\n          axis.ticks=element_blank(),\r\n          legend.position=\"bottom\", \r\n          axis.title=element_text(size=rel(0.8), family=\"Source Sans Pro Semibold\"),\r\n          strip.text=element_text(size=rel(1), family=\"Source Sans Pro Semibold\"),\r\n          strip.background=element_rect(fill=\"#ffffff\", colour=NA),\r\n          panel.spacing.y=unit(1.5, \"lines\"),\r\n          legend.key = element_blank())\r\n  \r\n  ret\r\n}\r\n\r\n# Calculate where to put the dotted lines that show up every three entries\r\nx.breaks <- seq(length(employee_records_startdate$Fullname) + 0.5 - 3, 0, by=-3)\r\n\r\n# Build plot\r\ntimeline_p2 <- ggplot(employee_records_startdate.long, \r\n                      aes(x=Fullname, y=employee_records_startdate.date, colour=CurrentEmploymentType)) + \r\n                      geom_line(size=6) + \r\n                      geom_vline(xintercept=x.breaks, colour=\"grey80\", linetype=\"dotted\") + \r\n                      guides(colour=guide_legend(title=NULL)) +\r\n                      labs(x=NULL, y=NULL) + coord_flip() +\r\n                      scale_y_date(date_breaks=\"1 year\", date_labels = (\"%Y\")) +\r\n                      theme_economist() + scale_color_brewer(palette = \"Dark2\") + \r\n                      theme(axis.text.x=element_text(angle=0, vjust = 1, size = 12, face = 'bold'), \r\n                            axis.text.y = element_text(size = 12, face = 'bold'), \r\n                            legend.text = element_text(size=12)) + ggtitle(\"Employment Period\") + \r\n                      theme(plot.title = element_text(hjust = 0.5, size = 18, color = \"darkblue\"))\r\n\r\ntimeline_p2  \r\n\r\n\r\n\r\nPatchwork allows the plots to be joined together in a figure.\r\nUsing patchwork:\r\n\r\n\r\npatchwork <- timeline_p2 / timeline_Kronosmilitary\r\n\r\npatchwork + \r\n  plot_annotation(title = 'Timeline graphs of GAStech employees with suspicious email',\r\n                  theme = theme(plot.title = element_text(size = 18))) & \r\n  theme(text = element_text('Roboto Condensed'))\r\n\r\n\r\n\r\nReferences\r\nhttp://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC1%20-%20Disappearance%20at%20GASTech/entries/Peking%20University/\r\nhttp://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC1%20-%20Disappearance%20at%20GASTech/entries/Georgia%20Institute%20of%20Technology/\r\nhttp://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC1%20-%20Disappearance%20at%20GASTech/entries/Tianjin%20University%20-%20Gao/\r\nhttps://knowledger.rbind.io/post/topic-modeling-using-r/\r\nhttps://kgjerde.github.io/corporaexplorer/articles/jane_austen.html\r\nhttps://ldavis.cpsievert.me/reviews/reviews.html\r\nhttps://rpubs.com/cosmopolitanvan/topicmodeling\r\nhttps://bookdown.org/yih_huynh/Guide-to-R-Book/tidyverse.html\r\n\r\nhttps://themilitarywallet.com/types-of-military-discharges/↩︎\r\n",
    "preview": "posts/2021-07-23-vast-challenge-2021-mini-challenge-1/img/Relationships between suspects and org.PNG",
    "last_modified": "2021-07-25T20:14:25+08:00",
    "input_file": "vast-challenge-2021-mini-challenge-1.knit.md"
  }
]
